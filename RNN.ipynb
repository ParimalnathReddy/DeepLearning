{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/mnt/home/kodumuru/ITM891_Parimal/ITM891_PROJECT/SAP/DL/GData.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_CASE_KEY                            0\n",
      "EBELN                                 0\n",
      "EBELP                                 0\n",
      "createtime                            0\n",
      "firstreceivetime                      0\n",
      "changeconfirmeddeliverydate           0\n",
      "changecontract                        0\n",
      "changecurrency                        0\n",
      "changedeliveryindicator               0\n",
      "changefinalinvoiceindicator           0\n",
      "changeoutwarddeliveryindicator        0\n",
      "changeprice                           0\n",
      "changequantity                        0\n",
      "changerequesteddeliverydate           0\n",
      "changestoragelocation                 0\n",
      "numdelivery                           0\n",
      "GDdays                                4\n",
      "BUKRS                                 0\n",
      "MATKL                                 0\n",
      "MATNR                             56390\n",
      "NETPR                                 0\n",
      "PSTYP                                 0\n",
      "WERKS                                 0\n",
      "ERNAM                             39073\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show me the null values\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the null value column of MATNR\n",
    "data = data.dropna(subset=['MATNR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill ERNAM with nan\n",
    "data['ERNAM'] = data['ERNAM'].fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_CASE_KEY                        0\n",
      "EBELN                             0\n",
      "EBELP                             0\n",
      "createtime                        0\n",
      "firstreceivetime                  0\n",
      "changeconfirmeddeliverydate       0\n",
      "changecontract                    0\n",
      "changecurrency                    0\n",
      "changedeliveryindicator           0\n",
      "changefinalinvoiceindicator       0\n",
      "changeoutwarddeliveryindicator    0\n",
      "changeprice                       0\n",
      "changequantity                    0\n",
      "changerequesteddeliverydate       0\n",
      "changestoragelocation             0\n",
      "numdelivery                       0\n",
      "GDdays                            4\n",
      "BUKRS                             0\n",
      "MATKL                             0\n",
      "MATNR                             0\n",
      "NETPR                             0\n",
      "PSTYP                             0\n",
      "WERKS                             0\n",
      "ERNAM                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show me the columns with null values\n",
    "print(data.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill GDdays missing values with mean\n",
    "data['GDdays'] = data['GDdays'].fillna(data['GDdays'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_CASE_KEY                        0\n",
      "EBELN                             0\n",
      "EBELP                             0\n",
      "createtime                        0\n",
      "firstreceivetime                  0\n",
      "changeconfirmeddeliverydate       0\n",
      "changecontract                    0\n",
      "changecurrency                    0\n",
      "changedeliveryindicator           0\n",
      "changefinalinvoiceindicator       0\n",
      "changeoutwarddeliveryindicator    0\n",
      "changeprice                       0\n",
      "changequantity                    0\n",
      "changerequesteddeliverydate       0\n",
      "changestoragelocation             0\n",
      "numdelivery                       0\n",
      "GDdays                            0\n",
      "BUKRS                             0\n",
      "MATKL                             0\n",
      "MATNR                             0\n",
      "NETPR                             0\n",
      "PSTYP                             0\n",
      "WERKS                             0\n",
      "ERNAM                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show me the null values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Convert Date-Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['createtime', 'firstreceivetime', 'changeconfirmeddeliverydate', 'changerequesteddeliverydate']\n",
    "for col in date_columns:\n",
    "    data[col] = pd.to_datetime(data[col])\n",
    "    data[col] = data[col].astype(int) / 10**9  # convert to seconds since epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup transformers for numerical and categorical data\n",
    "numeric_features = ['EBELP', 'numdelivery', 'NETPR'] + date_columns  # add other numeric columns as necessary\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['EBELN', 'changecontract', 'changecurrency', 'changedeliveryindicator', \n",
    "                        'changefinalinvoiceindicator', 'changeoutwarddeliveryindicator', 'changeprice', \n",
    "                        'changequantity', 'changestoragelocation', 'BUKRS', 'MATKL', 'MATNR', 'PSTYP', 'WERKS', 'ERNAM']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = data.drop('GDdays', axis=1)  # assuming 'GDdays' is the target\n",
    "y = data['GDdays']\n",
    "\n",
    "# Apply the preprocessing pipeline to the features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'GDdays' is your target variable\n",
    "features = ['EBELN', 'EBELP', 'createtime', 'firstreceivetime', 'changeconfirmeddeliverydate', \n",
    "            'changecontract', 'changecurrency', 'changedeliveryindicator', 'changefinalinvoiceindicator', \n",
    "            'changeoutwarddeliveryindicator', 'changeprice', 'changequantity', 'changerequesteddeliverydate', \n",
    "            'changestoragelocation', 'numdelivery', 'BUKRS', 'MATKL', 'MATNR', 'NETPR', 'PSTYP', 'WERKS', 'ERNAM']\n",
    "target = 'GDdays'\n",
    "\n",
    "# Convert date-time columns to numerical by extracting UNIX timestamp\n",
    "date_cols = ['createtime', 'firstreceivetime', 'changeconfirmeddeliverydate', 'changerequesteddeliverydate']\n",
    "for col in date_cols:\n",
    "    data[col] = pd.to_datetime(data[col]).astype(int) / 10**9\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_features = ['EBELP', 'NETPR', 'numdelivery'] + date_cols\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_features = ['EBELN', 'changecontract', 'changecurrency', 'changedeliveryindicator', \n",
    "                        'changefinalinvoiceindicator', 'changeoutwarddeliveryindicator', 'changeprice', \n",
    "                        'changequantity', 'changestoragelocation', 'BUKRS', 'MATKL', 'MATNR', 'PSTYP', 'WERKS', 'ERNAM']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = data[features]\n",
    "y = data[target].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply transformations\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Ensure input is 3D for LSTM\n",
    "X_train = X_train.toarray().reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.toarray().reshape((X_test.shape[0], 1, X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Building and Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16017 samples, validate on 4005 samples\n",
      "Epoch 1/10\n",
      "16017/16017 [==============================] - 14s 902us/step - loss: 1794.2048 - val_loss: 1712.5598\n",
      "Epoch 2/10\n",
      "16017/16017 [==============================] - 12s 756us/step - loss: 1509.2962 - val_loss: 1513.5467\n",
      "Epoch 3/10\n",
      "16017/16017 [==============================] - 12s 755us/step - loss: 1289.4262 - val_loss: 1285.8686\n",
      "Epoch 4/10\n",
      "16017/16017 [==============================] - 12s 756us/step - loss: 1117.0290 - val_loss: 1125.4311\n",
      "Epoch 5/10\n",
      "16017/16017 [==============================] - 12s 759us/step - loss: 980.1430 - val_loss: 992.2624\n",
      "Epoch 6/10\n",
      "16017/16017 [==============================] - 12s 768us/step - loss: 869.5082 - val_loss: 880.0173\n",
      "Epoch 7/10\n",
      "16017/16017 [==============================] - 12s 762us/step - loss: 773.2968 - val_loss: 783.6054\n",
      "Epoch 8/10\n",
      "16017/16017 [==============================] - 12s 758us/step - loss: 689.0111 - val_loss: 699.7349\n",
      "Epoch 9/10\n",
      "16017/16017 [==============================] - 12s 758us/step - loss: 616.7570 - val_loss: 624.2470\n",
      "Epoch 10/10\n",
      "16017/16017 [==============================] - 12s 759us/step - loss: 552.2468 - val_loss: 558.8674\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(30, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numba' has no attribute 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-59fc389cde16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# It's good practice to use a background dataset to approximate the expected value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"As of version 0.29.0 shap only supports Python 3 (not 2)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCohorts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/_explanation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslicer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSlicer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# from ._order import Order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimensionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhclust_ordering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_tree_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_minimization_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhclust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapproximate_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotential_interactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_import_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshapley_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordinal_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress_stderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_show_progress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_masked_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/shap/utils/_clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Re-export types itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Re-export all type names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numba' has no attribute 'core'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# It's good practice to use a background dataset to approximate the expected value\n",
    "background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]\n",
    "\n",
    "# SHAP Deep Explainer\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "shap_values = explainer.shap_values(X_test[:10])  # Explain a subset of predictions\n",
    "\n",
    "# Plot the SHAP values for the first prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], features=X_test[0], feature_names=features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
